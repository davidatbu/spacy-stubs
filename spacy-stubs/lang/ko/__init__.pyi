from ...language import Language
from ...util import DummyTokenizer
from typing import Any, Optional

class KoreanTokenizer(DummyTokenizer):
    vocab: Any = ...
    Tokenizer: Any = ...
    def __init__(self, cls: Any, nlp: Optional[Any] = ...) -> None: ...
    def __call__(self, text: Any): ...
    def detailed_tokens(self, text: Any) -> None: ...

class KoreanDefaults(Language.Defaults):
    lex_attr_getters: Any = ...
    stop_words: Any = ...
    tag_map: Any = ...
    writing_system: Any = ...
    @classmethod
    def create_tokenizer(cls, nlp: Optional[Any] = ...): ...

class Korean(Language):
    lang: str = ...
    Defaults: Any = ...
    def make_doc(self, text: Any): ...
