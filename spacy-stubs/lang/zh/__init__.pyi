from ...language import Language
from ...util import DummyTokenizer
from typing import Any, Optional

class ChineseTokenizer(DummyTokenizer):
    vocab: Any = ...
    use_jieba: Any = ...
    jieba_seg: Any = ...
    tokenizer: Any = ...
    def __init__(self, cls: Any, nlp: Optional[Any] = ...) -> None: ...
    def __call__(self, text: Any): ...

class ChineseDefaults(Language.Defaults):
    lex_attr_getters: Any = ...
    tokenizer_exceptions: Any = ...
    stop_words: Any = ...
    tag_map: Any = ...
    writing_system: Any = ...
    use_jieba: bool = ...
    @classmethod
    def create_tokenizer(cls, nlp: Optional[Any] = ...): ...

class Chinese(Language):
    lang: str = ...
    Defaults: Any = ...
    def make_doc(self, text: Any): ...
