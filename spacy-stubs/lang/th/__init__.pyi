from ...language import Language
from ...util import DummyTokenizer
from typing import Any, Optional

class ThaiTokenizer(DummyTokenizer):
    word_tokenize: Any = ...
    vocab: Any = ...
    def __init__(self, cls: Any, nlp: Optional[Any] = ...) -> None: ...
    def __call__(self, text: Any): ...

class ThaiDefaults(Language.Defaults):
    lex_attr_getters: Any = ...
    tokenizer_exceptions: Any = ...
    tag_map: Any = ...
    stop_words: Any = ...
    @classmethod
    def create_tokenizer(cls, nlp: Optional[Any] = ...): ...

class Thai(Language):
    lang: str = ...
    Defaults: Any = ...
    def make_doc(self, text: Any): ...
